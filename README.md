# Autonomous-Vehicles-Survey


## Papers

### End-to-End AD 
* World4Drive: End-to-End Autonomous Driving via Intention-aware Physical Latent World Model [Paper](https://arxiv.org/abs/2507.00603)
* WoTE: End-to-End Driving with Online Trajectory Evaluation via BEV World Model [Paper](https://arxiv.org/abs/2504.01941), [Code](https://github.com/liyingyanUCAS/WoTE)
 * MomAD: Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous Driving ; [Paper](https://arxiv.org/abs/2503.03125), [Code](https://github.com/adept-thu/MomAD), [Website](https://4dvlab.github.io/project_page/realad)

* LAW: Enhancing End-to-End Autonomous Driving with Latent World Model [Paper](https://openreview.net/pdf?id=fd2u60ryG0), [Code](https://github.com/BraveGroup/LAW)
* SSR: Navigation-Guided Sparse Scene Representation for End-to-End Autonomous Driving [Paper](https://openreview.net/pdf?id=Vv76fCYffN), [Code](https://github.com/PeidongLi/SSR)
  
* RoCA: Robust Cross-Domain End-to-End Autonomous Driving [Paper](https://arxiv.org/pdf/2506.10145)
* DriveSuprim: Towards Precise Trajectory Selection for End-to-End Planning [Paper](https://arxiv.org/pdf/2506.06659)

* iPad: Iterative Proposal-centric End-to-End Autonomous Driving [Paper](https://arxiv.org/pdf/2505.15111), [Code](https://github.com/Kguo-cs/iPad)

* StyleDrive: Towards Driving-Style Aware Benchmarking of End-To-End Autonomous Driving [Paper](https://arxiv.org/pdf/2506.23982), [Code](https://styledrive.github.io/)
* Echo Planning for Autonomous Driving: From Current Observations to Future Trajectories and Back [Paper](https://arxiv.org/pdf/2505.18945)

* Perception in Plan: Coupled Perception and Planning for End-to-End Autonomous Driving [Paper](https://arxiv.org/pdf/2508.11488), [Code](https://github.com/fudan-zvg/VeteranAD)

* SpaRC-AD: A Baseline for Radar-Camera Fusion in End-to-End Autonomous Driving  [Paper](https://arxiv.org/pdf/2508.10567), [Code](https://phi-wol.github.io/sparcad/)
* DistillDrive: End-to-End Multi-Mode Autonomous Driving Distillation by Isomorphic Hetero-Source Planning Model  [Paper](https://arxiv.org/pdf/2508.05402), [Code](https://github.com/YuruiAI/DistillDrive)
* SKGE-SWIN: End-To-End Autonomous Vehicle Waypoint Prediction and Navigation Using Skip Stage Swin Transformer  [Paper](https://arxiv.org/abs/2508.20762), [Code](https://github.com/fachrinnk4869/skge-swin)
* ReAL-AD: Towards Human-Like Reasoning in End-to-End Autonomous Driving [Paper](https://arxiv.org/pdf/2507.12499)

* Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving  [Paper](https://arxiv.org/pdf/2406.03877), [Website](https://thinklab-sjtu.github.io/Bench2Drive/), [Code](https://github.com/Thinklab-SJTU/Bench2Drive)

* Bridging the Domain Gap between Synthetic and Real-World Data for Autonomous Driving [Paper](https://dl.acm.org/doi/pdf/10.1145/3633463), [Code](https://github.com/ostadabbas/SAVeS-Platform)

* BridgeAD: Bridging Past and Future: End-to-End Autonomous Driving with Historical Prediction and Planning [Paper](https://openaccess.thecvf.com/content/CVPR2025/papers/Zhang_Bridging_Past_and_Future_End-to-End_Autonomous_Driving_with_Historical_Prediction_CVPR_2025_paper.pdf?utm_source=chatgpt.com), [Code](https://github.com/fudan-zvg/BridgeAD)

* GenAD: Generative End-to-End Autonomous Driving [Paper](https://arxiv.org/pdf/2402.11502), [Code](https://github.com/wzzheng/GenAD)



### Surveys
* A Survey of World Models for Autonomous Driving [Paper](https://arxiv.org/abs/2501.11260)
* End-to-End Autonomous Driving: Challenges and Frontiers [paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10614862)
* World Models for Autonomous Driving: An Initial Survey  [Paper](https://arxiv.org/abs/2403.02622)
* Data-Centric Evolution in Autonomous Driving: A Comprehensive Survey of Big
Data System, Data Mining, and Closed-Loop Technologies [Paper](https://arxiv.org/pdf/2401.12888.pdf)
* Forging Vision Foundation Models for Autonomous Driving: Challenges, Methodologies, and Opportunities  [Paper](https://arxiv.org/pdf/2401.08045.pdf)
* A Survey for Foundation Models in Autonomous Driving [Paper](https://arxiv.org/pdf/2402.01105)





### World Models
* UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs [Paper](https://arxiv.org/pdf/2511.01768), [Code](https://github.com/happinesslz/UniLION)
* DINO-Foresight: Looking into the Future with DINO [Paper](https://arxiv.org/abs/2412.11673), [Code](https://github.com/Sta8is/DINO-Foresight)

  * Genesis: Multimodal Driving Scene Generation with Spatio-Temporal and Cross-Modal Consistency [Paper](https://arxiv.org/abs/2506.07497), [website](https://arxiv.org/abs/2506.07497), [Code to be released](https://github.com/xiaomi-research/genesis)

  * AdaWM: Adaptive World Model-based Planning for Autonomous Driving [Paper](https://openreview.net/pdf?id=NEu8wgPctU)
 

* AD-L-JEPA: Self-Supervised Spatial World Models with Joint Embedding Predictive Architecture for Autonomous Driving with LiDAR Data.[Paper](https://arxiv.org/abs/2501.04969), [Code](https://github.com/HaoranZhuExplorer/AD-L-JEPA-Release)

* 2025-V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction and Planning  [Paper](https://ai.meta.com/research/publications/v-jepa-2-self-supervised-video-models-enable-understanding-prediction-and-planning/), [Code](https://github.com/facebookresearch/vjepa2)
  * 2024-V-JEPA: Revisiting Feature Prediction for Learning Visual Representations from Video [Paper](https://ai.meta.com/research/publications/revisiting-feature-prediction-for-learning-visual-representations-from-video/), [Code](https://github.com/facebookresearch/jepa)


* 2023-I-JEPA: Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture [Paper](https://arxiv.org/abs/2301.08243), [Code](https://github.com/facebookresearch/ijepa)



  
### Vision Language Models

* Vision Language Models in Autonomous Driving: A Survey and Outlook [Paper](https://arxiv.org/pdf/2310.14414)
* A Survey on Efficient Vision-Language Models [Paper](https://arxiv.org/pdf/2504.09724)

* A Comprehensive Survey and Guide to Multimodal Large Language Models in Vision-Language Tasks [Paper](https://arxiv.org/pdf/2411.06284)




### Vision Language Action Models
* A Survey on Vision-Language-Action Models for Autonomous Driving [Paper](https://arxiv.org/pdf/2506.24044)
  
* 2025-UniVLA: Learning to Act Anywhere with Task-centric Latent Actions  [Paper](https://arxiv.org/abs/2505.06111), [Code](https://github.com/OpenDriveLab/UniVLA)

